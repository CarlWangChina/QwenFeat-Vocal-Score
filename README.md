# VocalVerse Project Overview

This work sincerely acknowledges the support of previous works such as QwenAudio, SongEval, and MuQ.

## ğŸ“œ License & Copyright

1. **License Framework**: This project is licensed under the **[CC BY-NC-ND 4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/) (Attribution-NonCommercial-NoDerivs 4.0 International)** license.
2. **Non-Commercial Use**: The code, model weights, and documentation in this project are free for academic exchange and personal educational purposes only.
3. **Commercial Use Strictly Prohibited**: Without written authorization, it is strictly forbidden to use any part of this project for any form of commercial profit (including but not limited to integration into commercial software, providing commercial AI services, etc.).
4. **Commercial Licensing Inquiries**: For commercial cooperation or to obtain a commercial license, please contact: **3156018231@qq.com**.

---

## ğŸŒ Version Notice & Legacy Archive

**Legacy Repository**:  
[https://github.com/CarlWangChina/Singing-Aesthetic-Assessment](https://github.com/CarlWangChina/Singing-Aesthetic-Assessment)


## ğŸ“– Paper Citation & Instructions

For detailed technical solutions, experimental results, and theoretical support regarding this implementation, please refer to our research paper. If you use the code or models from this repository in your research or work, please cite our paper.

### Paper Information

**Title**: Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model
**Conference**: Proceedings of the 33rd ACM International Conference on Multimedia (MM '25)

* **ACM Digital Library (Official Version)**: [https://doi.org/10.1145/3746027.3758148](https://doi.org/10.1145/3746027.3758148)
* **arXiv (Free Preview Version)**: [https://www.arxiv.org/abs/2512.06999](https://www.arxiv.org/abs/2512.06999) 
  *(Note: The arXiv version is identical in content to the official ACM DL version)*

### Citation Format

#### ACM Reference Format
> Zihao Wang, Ruibin Yuan, Ziqi Geng, Hengjia Li, Xingwei Qu, Xinyi Li, Songye Chen, Haoying Fu, Roger B. Dannenberg, and Kejun Zhang. 2025. Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model. In Proceedings of the 33rd ACM International Conference on Multimedia (MM '25). Association for Computing Machinery, New York, NY, USA, 12227â€“12236. https://doi.org/10.1145/3746027.3758148

#### BibTeX
```bibtex
@inproceedings{10.1145/3746027.3758148,
author = {Wang, Zihao and Yuan, Ruibin and Geng, Ziqi and Li, Hengjia ox and Qu, Xingwei and Li, Xinyi and Chen, Songye and Fu, Haoying and Dannenberg, Roger B. and Zhang, Kejun},
title = {Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {[https://doi.org/10.1145/3746027.3758148](https://doi.org/10.1145/3746027.3758148)},
doi = {10.1145/3746027.3758148},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {12227â€“12236},
numpages = {10},
keywords = {computational music aesthetics, descriptive feedback, multi-dimensional evaluation, multimodal foundation models, singing timbre popularity, singing voice assessment},
location = {Dublin, Ireland},
series = {MM '25}
}
```

## ğŸ“‚ Data Description

The VocalVerse_Datasets-human_labels folder contains three core files, providing two-level evaluation data (amateur and professional) for singing performance.

| File Name | Description |
| :--- | :--- |
| **`Amateur_overall_mos_avg5.xlsx`** | **Amateur Consensus Scores:** Contains overall "pleasantness" ratings from 165 amateur annotators. Each recording was rated by 5 independent annotators on a 1-5 Likert scale, and this file provides the final Mean Opinion Scores (MOS). |
| **`Professional_multidim_annotations_raw_...xlsx`** | **Expert Multi-dimensional Labels:** Detailed annotations provided by two professional vocal coaches. It includes 1-5 integer scores across four core dimensionsâ€”**Timbre, Breath, Emotion, and Technique**â€”along with accompanying **textual critiques**. |
| **`Professional_scoring_rubric.xlsx`** | **Scoring Standards:** The formal criteria (Rubric) used by experts to ensure consistency and high quality across the four dimensions. |

### Data Scale

According to the research paper:
* **Original Data Pool:** We initially collected over **100,000** raw a cappella recordings.
* **Pre-screened Set:** Following automated preliminary screening via manual and RuleSignal scoring, a dataset of **10,000** clips was formed.
* **Current Open-source Subset:** The files provided in this repository represent the **top 10% (approximately 1,000 recordings with high technical proficiency)**, which were subsequently subjected to intensive professional multi-dimensional annotation.

### Annotation Methodology

1. **Amateur Phase:** A total of 165 non-music major annotators participated. They employed a "forced distribution" method to ensure score differentiation and capture general aesthetic preferences.
2. **Professional Phase:** Two senior vocal teachers provided dual-modality annotations (scores + descriptive text) to support the training of descriptive Multimodal Large Language Models (MLLMs).
3. **Evaluation Dimensions:**
    * **Timbre Quality:** The uniqueness, texture, and layering of the voice.
    * **Breath Control:** Support and stability for complex phrases.
    * **Emotional Expression:** The infectiousness and resonance of the performance.
    * **Vocal Technique:** Mastery of singing skills.


# VocalVerse1: Singing Evaluation Model based on QwenAudio

[qwenaudio](./qwenaudio/README.md) **Qwen Comment Generation + Scoring Module**: This includes a Lora-trained version of Qwen-audio. It takes audio as input and outputs comments on issues in the singing voice (equivalent to descriptive tagging). These comments are then used as input for a "deep thinking" phase to perform the final timbre scoring. Finally, a TTS system with a singer's voice is used to generate the vocal critique. The full workflow is:

1. The fine-tuned Qwen model generates comments on singing issues.
2. Comments and audio are fed back into the Qwen scoring model to assist in evaluation (acting as a "deep thinking" step).
3. Another LLM call polishes the comments, generates a summary, and provides vocal suggestions.
4. Finally, the summary is read aloud using the corresponding singer's voice.

All weights for this section are preserved. Model directory link: Download the full repository containing models from Hugging Face: [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/] 


## Usage

### Download Models
Download the following directory and place it in the current path:
https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/tree/main/qwenaudio/ckpts
Or simply run: `git clone https://huggingface.co/karl-wang/QwenFeat-Vocal-Score`

### Start Service

`python scripts/infer_service.py`  
Call via CURL:  
`curl -X POST http://localhost:8080/score -F "file=@/path/to/audio"`

### CLI Usage
`python scripts/infer.py path_to_audio.wav output_file.txt`

### Python API

Refer to `tests/test_score.py` for implementation.

```python
import qwenaudio.processor
import qwenaudio.prompts
import librosa

# Initialize model and processor
processor = qwenaudio.processor.create_processor(
            "ckpts/generator-lora-32-16-scoreonly-f16/best_model_epoch_13/lora_weights",
            "ckpts/generator-lora-32-16-textonly-simple-v2-int4/best_model_epoch_16/lora_weights"
        )

# Scoring
audio_path = "/home/w-4090/cutted_score_audio_separated/446892/344523004.wav"
data, sr = librosa.load(audio_path, sr=processor.processor.feature_extractor.sampling_rate, mono=True)
data = data[:processor.processor.feature_extractor.sampling_rate * 30] # Use first 30 seconds
print(data.shape)

for i in range(4):
    print("test:", qwenaudio.prompts.prompt_mapper_reverse[i]) # Meanings of parameters 0-4
    score = processor.generate(data, i) # Generate score
```

If model loading hangs, try using a Hugging Face mirror:
`export HF_ENDPOINT=https://hf-mirror.com`

# VocalVerse2: Vocal Recording Scoring Model based on MuQ

[audioscore](./audioscore/README.md) **MuQ Scoring & Ranking Module**: Contains two versions (with and without decoupling) using the same codebase, organized into a single directory.

1. **Scoring using MuQ as encoder + scoring head**: No decoupling. This architecture is basically the same as the SongEval work. The unfrozen MuQ Lora weights and the scoring head weights are preserved. Link: [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/] 

2. **Decoupling Experiment**: Uses the speaker encoder from SaMoye-SVC for reverse gradient training to decouple speaker identity features, aiming to improve aesthetic understanding accuracy. The code is compatible with both versions. Note: Decoupling weights were lost due to a server move. However, experiments using SaMoyeâ€™s or Wespeakerâ€™s encoder for reverse gradient training showed that while training and convergence are difficult (easier with smaller batch sizes), the final aesthetic assessment accuracy slightly improved, proving that decoupling is effective.

**Note**: This repository does not contain the model weights. Researchers should download the full repository including models from Hugging Face: [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/] 

## Usage

### Environment Setup

```bash
conda create -n audioscore python=3.10
conda activate audioscore
pip install -r requirements.txt
```

### Download Models
Download the following directory and place it in the current path:
https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/tree/main/audioscore/ckpts
Or `git clone https://huggingface.co/karl-wang/QwenFeat-Vocal-Score`

### Inference
(Runnable code is available in `python tests/test_generate_score.py`)

```python
import os, sys
import audioscore.model

if __name__ == "__main__":
    ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model = audioscore.model.SongEvalGenerator_audio_lora()
    # Load model
    model.load_model(os.path.join(ROOT_DIR, "ckpts", "SongEvalGenerator", "step_2_al_audio", "best_model_step_132000")) 
    model = model.half() # Optional
    model = model.cuda()

    # Perform scoring
    score = model.generate_tag("/data/nfs/audioscore/data/sort/data/audio/203887_250822105518005501.m4a")

    print("score:", score)
```

Run tests directly:
```bash
python tests/test_generate_score.py
```

### Training

Direct Training:
`torchrun --nproc_per_node=4 --nnodes=1 scripts/train/train_sort_audio.py`

Adversarial Training (Decoupling with SaMoye spk encoder):
`torchrun --nproc_per_node=4 --nnodes=1 scripts/train/train_sort_audio_grl.py`

## Additional Notes for VocalVerse1 (QwenAudio based)

- Use the `qwenaudio` conda environment; weights for both models have been included.
- Added `infer_service.py` and `infer.py` scripts; instructions are in the README. `test/test_score.py` is functional.
- The prompts are located at `/home/w-4090/projects/qwenaudio/src/qwenaudio/prompts.py`.
- Test scripts are available at `/home/w-4090/projects/qwenaudio/tests/test_processor_v2.py` and `/home/w-4090/projects/qwenaudio/tests/test_processor_v3.py`.
- For the code in Figures 1 and 2: when loading models locally, change the model paths in `model.py` and `processor.py` to your local paths.

<img width="1601" height="315" alt="77b6f8227c1d4af030fd7e2e0af8c8ab" src="https://github.com/user-attachments/assets/191e8e3f-2b67-43c0-b51e-f0b50f5d7dd6" />
<img width="1335" height="165" alt="76943b4c05d12bf090aecc602c8430b4" src="https://github.com/user-attachments/assets/8b058f65-de12-44c6-b041-e7db5ef97095" />

- Figure 3 shows the two trained Lora models. These are ready and should be uploaded.
<img width="418" height="56" alt="image" src="https://github.com/user-attachments/assets/e07494e7-eedc-44bc-b885-bf825de3d3dc" />

- The entire project including code and Lora weights is included, but the base model must be downloaded from the internet.
- If only the final classifier is active and the Lora on the model body is not, accuracy will be very low. Once Lora weights are active, accuracy reaches 80%-90%. (Encoder + LLM Decode + Lora + Classifier).
- Figure 4 shows that the `Qwen2-Audio-7B-Instruct/` base model is required. It was previously downloaded automatically to `.cache`; if downloaded manually, re-specify the path in the code.

<img width="785" height="858" alt="32e73d7a89338a602f350225e859b67f" src="https://github.com/user-attachments/assets/26b3f4f9-2595-4567-934d-10d089785464" />

# VocalVerseé¡¹ç›®æ€»è§ˆ

æœ¬å·¥ä½œéƒ‘é‡æ„Ÿè°¢ QwenAudio, SongEval, MuQç­‰å…ˆå‰å·¥ä½œçš„æ”¯æŒ.

## ğŸ“œ è®¸å¯åè®®ä¸ç‰ˆæƒå£°æ˜ | License & Copyright

1. **åè®®æ¡†æ¶**ï¼šæœ¬é¡¹ç›®é‡‡ç”¨ **[CC BY-NC-ND 4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/) (ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç»)** å›½é™…è®¸å¯åè®®ã€‚
2. **éå•†ä¸šç”¨é€”**ï¼šæœ¬é¡¹ç›®ä»£ç ã€æ¨¡å‹æƒé‡åŠæ–‡æ¡£ä»…é™å­¦æœ¯äº¤æµå’Œä¸ªäººæ•™è‚²ç”¨é€”å…è´¹ä½¿ç”¨ã€‚
3. **ä¸¥ç¦å•†ç”¨**ï¼šæœªç»ä¹¦é¢æˆæƒï¼Œä¸¥ç¦å°†æœ¬é¡¹ç›®ä»»ä½•éƒ¨åˆ†ç”¨äºä»»ä½•å½¢å¼çš„å•†ä¸šè¥åˆ©ç›®çš„ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºé›†æˆè‡³å•†ä¸šè½¯ä»¶ã€æä¾›è¥åˆ©æ€§ AI æœåŠ¡ç­‰ï¼‰ã€‚
4. **å•†ä¸šæˆæƒç”³è¯·**ï¼šå¦‚æœ‰å•†ä¸šåˆä½œæ„å‘æˆ–éœ€è·å¾—å•†ä¸šä½¿ç”¨è®¸å¯ï¼Œè¯·åŠ¡å¿…è”ç³»é‚®ç®±ï¼š**3156018231@qq.com**ã€‚
---

## ğŸŒ ç‰ˆæœ¬è¯´æ˜ä¸æ—§ç‰ˆå­˜æ¡£ | Version Notice & Legacy Archive

**Legacy Repository / æ—§ç‰ˆé¡¹ç›®åœ°å€**: 
[https://github.com/CarlWangChina/Singing-Aesthetic-Assessment](https://github.com/CarlWangChina/Singing-Aesthetic-Assessment)


## ğŸ“– è®ºæ–‡å¼•ç”¨ä¸è¯´æ˜

å…³äºæœ¬ä»£ç å®ç°çš„è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆã€å®éªŒç»“æœåŠç†è®ºæ”¯æ’‘ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ç ”ç©¶è®ºæ–‡ã€‚å¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–å·¥ä½œä¸­ä½¿ç”¨äº†æœ¬ä»“åº“çš„ä»£ç æˆ–æ¨¡å‹ï¼Œæ¬¢è¿å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

### è®ºæ–‡ä¿¡æ¯

**æ ‡é¢˜**ï¼šSinging Timbre Popularity Assessment Based on Multimodal Large Foundation Model
**ä¼šè®®**ï¼šProceedings of the 33rd ACM International Conference on Multimedia (MM '25)

* **ACM Digital Library (å®˜æ–¹ç‰ˆæœ¬)**: [https://doi.org/10.1145/3746027.3758148](https://doi.org/10.1145/3746027.3758148)
* **arXiv (å…è´¹é¢„è§ˆç‰ˆ)**: [https://www.arxiv.org/abs/2512.06999](https://www.arxiv.org/abs/2512.06999) 
  *(æ³¨ï¼šarXiv ç‰ˆæœ¬å†…å®¹ä¸ ACM DL å®˜æ–¹ç‰ˆæœ¬å®Œå…¨ä¸€è‡´)*

### å¼•ç”¨æ ¼å¼

#### ACM Reference Format
> Zihao Wang, Ruibin Yuan, Ziqi Geng, Hengjia Li, Xingwei Qu, Xinyi Li, Songye Chen, Haoying Fu, Roger B. Dannenberg, and Kejun Zhang. 2025. Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model. In Proceedings of the 33rd ACM International Conference on Multimedia (MM '25). Association for Computing Machinery, New York, NY, USA, 12227â€“12236. https://doi.org/10.1145/3746027.3758148

#### BibTeX
```bibtex
@inproceedings{10.1145/3746027.3758148,
author = {Wang, Zihao and Yuan, Ruibin and Geng, Ziqi and Li, Hengjia and Qu, Xingwei and Li, Xinyi and Chen, Songye and Fu, Haoying and Dannenberg, Roger B. and Zhang, Kejun},
title = {Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {[https://doi.org/10.1145/3746027.3758148](https://doi.org/10.1145/3746027.3758148)},
doi = {10.1145/3746027.3758148},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {12227â€“12236},
numpages = {10},
keywords = {computational music aesthetics, descriptive feedback, multi-dimensional evaluation, multimodal foundation models, singing timbre popularity, singing voice assessment},
location = {Dublin, Ireland},
series = {MM '25}
}
``` 
## ğŸ“‚ VocalVerseæ•°æ®é›†

VocalVerse_Datasets-human_labelsæ–‡ä»¶å¤¹åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼Œæä¾›äº†é’ˆå¯¹æ¼”å”±è¡¨ç°çš„ä¸šä½™å’Œä¸“ä¸šä¸¤çº§è¯„ä»·æ•°æ®ã€‚

| æ–‡ä»¶å | è¯´æ˜ |
| :--- | :--- |
| **`Amateur_overall_mos_avg5.xlsx`** | **ä¸šä½™å…±è¯†è¯„åˆ†ï¼š** åŒ…å« 165 åä¸šä½™æ ‡æ³¨è€…çš„æ•´ä½“â€œå¬æ„Ÿæ„‰æ‚¦åº¦â€è¯„åˆ†ã€‚æ¯æ®µå½•éŸ³ç”± 5 åç‹¬ç«‹çš„æ ‡æ³¨è€…æŒ‰ 1-5 åˆ† Likert é‡è¡¨æ‰“åˆ†ï¼Œè¯¥æ–‡ä»¶æä¾›æœ€ç»ˆçš„å¹³å‡æ„è§åˆ† (MOS)ã€‚ |
| **`Professional_multidim_annotations_raw_...xlsx`** | **ä¸“å®¶å¤šç»´åº¦æ ‡ç­¾ï¼š** ç”±ä¸¤åä¸“ä¸šå£°ä¹æ•™ç»ƒæä¾›çš„è¯¦ç»†æ ‡æ³¨ã€‚åŒ…å« **éŸ³è‰²ã€æ°”æ¯ã€æƒ…æ„Ÿã€æŠ€å·§** å››ä¸ªæ ¸å¿ƒç»´åº¦çš„ 1-5 åˆ†æ•´æ•°è¯„åˆ†åŠé…å¥—çš„**æ–‡å­—è¯„è¯­**ã€‚ |
| **`Professional_scoring_rubric.xlsx`** | **è¯„åˆ†æ ‡å‡†ï¼š** ä¸“å®¶æ ‡æ³¨æ—¶ä½¿ç”¨çš„æ­£å¼å‡†åˆ™ï¼ˆRubricï¼‰ï¼Œç¡®ä¿å››ä¸ªç»´åº¦çš„æ ‡æ³¨å…·æœ‰ä¸€è‡´æ€§å’Œé«˜è´¨é‡ã€‚ |

### æ•°æ®è§„æ¨¡

æ ¹æ®è®ºæ–‡ç ”ç©¶ï¼š
* **åŸå§‹æ•°æ®æ± ï¼š** æˆ‘ä»¬æœ€åˆæ”¶é›†äº†è¶…è¿‡ **100,000** æ®µåŸå§‹æ¸…å”±å½•éŸ³ã€‚
* **é¢„ç­›é€‰é›†åˆï¼š** ç»è¿‡äººå·¥å’ŒRuleSignalè¯„åˆ†è‡ªåŠ¨åˆæ­¥ç­›é€‰ï¼Œå½¢æˆäº†åŒ…å« **10,000** æ®µç‰‡æ®µçš„æ•°æ®é›†ã€‚
* **å½“å‰å¼€æºå­é›†ï¼š** æœ¬ä»“åº“æä¾›çš„æ–‡ä»¶ä»£è¡¨äº†å…¶ä¸­ **å‰ 10%ï¼ˆçº¦ 1,000 æ®µæ­Œå”±æŠ€æœ¯ç²¾æ¹›çš„å½•éŸ³ï¼‰**ï¼Œè¿™äº›å½•éŸ³éšå, ç»è¿‡äº†å¯†é›†çš„ä¸“ä¸šå¤šç»´åº¦æ ‡æ³¨ã€‚

### æ ‡æ³¨æ–¹æ³•

1.  **ä¸šä½™é˜¶æ®µï¼š** å…±æœ‰ 165 åééŸ³ä¹ä¸“ä¸šæ ‡æ³¨è€…å‚ä¸ã€‚ä»–ä»¬é‡‡ç”¨â€œå¼ºåˆ¶åˆ†å¸ƒæ³•â€ä»¥ç¡®ä¿åˆ†æ•°å…·æœ‰åŒºåˆ†åº¦ï¼Œä»è€Œæ•æ‰å¤§ä¼—çš„å®¡ç¾åå¥½ã€‚
2.  **ä¸“ä¸šé˜¶æ®µï¼š** ä¸¤åèµ„æ·±å£°ä¹æ•™å¸ˆæä¾›åŒæ¨¡æ€æ ‡æ³¨ï¼ˆåˆ†æ•° + æè¿°æ€§æ–‡å­—ï¼‰ï¼Œä»¥æ”¯æŒæè¿°æ€§å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰çš„è®­ç»ƒã€‚
3.  **è¯„ä»·ç»´åº¦ï¼š**
    * **éŸ³è‰² (Timbre Quality)ï¼š** å£°éŸ³çš„ç‹¬ç‰¹åº¦ã€è´¨æ„Ÿå’Œå±‚æ¬¡æ„Ÿã€‚
    * **æ°”æ¯ (Breath Control)ï¼š** å¯¹å¤æ‚å¥å­çš„æ”¯æ’‘åŠ›å’Œç¨³å®šæ€§ã€‚
    * **æƒ…æ„Ÿ (Emotional Expression)ï¼š** æ¼”å”±çš„æ„ŸæŸ“åŠ›ä¸å…±é¸£æ„Ÿã€‚
    * **æŠ€å·§ (Vocal Technique)ï¼š** æ¼”å”±æŠ€å·§çš„ç†Ÿç»ƒåº¦ã€‚

# VocalVerse1: åŸºäºqwenaudioçš„æ­Œå”±è¯„ä»·æ¨¡å‹

[qwenaudio](./qwenaudio/README.md) Qwenè¯„è¯­ç”Ÿæˆ+æ‰“åˆ†éƒ¨åˆ†. åŒ…å«è¾“å…¥éŸ³é¢‘ç»™Qwen-audioæˆ‘ä»¬ç”¨Loraè®­ç»ƒåçš„ç‰ˆæœ¬, è¾“å‡ºé’ˆå¯¹æ­Œå£°çš„å­˜åœ¨é—®é¢˜çš„è¯„è¯­ ç›¸å½“äºå¯¹éŸ³é¢‘åšæè¿°æ€§æ‰“æ ‡, ç„¶åè¯„è¯­å†ä½œä¸ºè¾“å…¥ä½œä¸ºæ·±åº¦æ€è€ƒéƒ¨åˆ†, æœ€ç»ˆè¿›è¡ŒéŸ³è‰²æ‰“åˆ†, ç„¶åç”¨æ­Œæ‰‹éŸ³è‰²çš„TTSå¿µå‡ºæ¥ç”Ÿæˆç‚¹è¯„è¯­éŸ³, è¿™ä¸€æ•´å¥—çš„æµç¨‹:

1.qwenå¾®è°ƒè®­ç»ƒåçš„æ¨¡å‹ç”Ÿæˆæ­Œå”±é—®é¢˜çš„è¯„è¯­.
2.è¯„è¯­+éŸ³é¢‘ä¸€èµ·å†æ¬¡ç»™qwenæ‰“åˆ†æ¨¡å‹æ¥è¾…åŠ©æ‰“åˆ†(ç›¸å½“äºåšäº†ä¸€æ¬¡æ·±åº¦æ€è€ƒ). 
3.æœ€åå†è°ƒä¸€æ¬¡å¤§æ¨¡å‹æ¥å¯¹è¯„è¯­è¿›è¡Œæ¶¦è‰²,  ç”Ÿæˆæ€»ç»“ , ç„¶åç»™å‡ºå”±æ³•å»ºè®®. 
4.åœ¨æœ€åæŠŠæ€»ç»“ç”¨å¯¹åº”æ­Œæ‰‹çš„å£°éŸ³å¿µå‡ºæ¥. 

è¿™ä¸€å—çš„æ‰€æœ‰æƒé‡éƒ½ä¿ç•™äº†, æ¨¡å‹å¯¹åº”ç›®å½•é“¾æ¥: å‰å¾€huggingfaceä¸‹è½½åŒ…å«æ¨¡å‹çš„å®Œæ•´ä»“åº“ï¼š [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/]  

## è°ƒç”¨æ–¹æ³•

### ä¸‹è½½æ¨¡å‹  
ä¸‹è½½ä¸‹é¢çš„ç›®å½•ï¼Œæ”¾åœ¨å½“å‰ä½ç½®  
https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/tree/main/qwenaudio/ckpts  
ç›´æ¥git clone [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score](https://huggingface.co/karl-wang/QwenFeat-Vocal-Score) ä¹Ÿå¯ä»¥

### å¯ç”¨æœåŠ¡  

`python scripts/infer_service.py`  
è°ƒç”¨ï¼š  
`curl -X POST http://localhost:8080/score   -F "file=@éŸ³é¢‘è·¯å¾„"`  

### å‘½ä»¤è¡Œè°ƒç”¨
`python scripts/infer.py éŸ³é¢‘è·¯å¾„.wav è¾“å‡ºæ–‡ä»¶.txt`

### pythonè°ƒç”¨

ä»£ç å‚è€ƒ `tests/test_score.py`

```python
import qwenaudio.processor
import qwenaudio.prompts
import librosa

# åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
processor = qwenaudio.processor.create_processor(
            "ckpts/generator-lora-32-16-scoreonly-f16/best_model_epoch_13/lora_weights",
            "ckpts/generator-lora-32-16-textonly-simple-v2-int4/best_model_epoch_16/lora_weights"
        )

# æ‰“åˆ†
audio_path= "/home/w-4090/cutted_score_audio_separated/446892/344523004.wav"
data, sr = librosa.load(audio_path, sr=processor.processor.feature_extractor.sampling_rate, mono=True)
data = data[:processor.processor.feature_extractor.sampling_rate * 30] # æˆªå–30ç§’
print(data.shape)

for i in range(4):
    print("test:", qwenaudio.prompts.prompt_mapper_reverse[i]) # è¾“å‡ºå‚æ•°0åˆ°4çš„å«ä¹‰
    score = processor.generate(data, i)# ç”Ÿæˆåˆ†æ•°
```

å¦‚æœåœ¨åŠ è½½æ¨¡å‹æ—¶å¡ä½ï¼Œå¯å¯ç”¨huggingfaceé•œåƒ
`export HF_ENDPOINT=https://hf-mirror.com`

# VocalVerse2: åŸºäºMuQçš„äººå£°å½•éŸ³æ‰“åˆ†æ¨¡å‹ 

[audioscore](./audioscore/README.md) MuQæ‰“åˆ†ã€æ’åºéƒ¨åˆ†. åŒ…å« åŠ è§£è€¦ å’Œ ä¸åŠ è§£è€¦ ä¸¤ä¸ªç‰ˆæœ¬, ä½¿ç”¨çš„åŒä¸€å¥—ä»£ç ï¼Œåªåˆ†äº†ä¸€ä¸ªç›®å½•. 

1ã€ä½¿ç”¨MuQä½œä¸ºencoder+åæ¥æ‰“åˆ†å™¨è¿›è¡Œæ‰“åˆ†çš„ä»£ç ,ä¸åŠ è§£è€¦,è¿™ä¸€å—çš„æ¶æ„è·ŸSongEvalå·¥ä½œåŸºæœ¬ç›¸åŒ.  å…¶ä¸­MuQè§£å†»ç”¨loraçš„æƒé‡ã€åé¢æ‰“åˆ†å™¨çš„æƒé‡, éƒ½ä¿ç•™äº†,æƒé‡å¯¹åº”ç›®å½•é“¾æ¥:  [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/]  

2ã€åŠ è§£è€¦ , é‡‡ç”¨SaMoye-SVCçš„ spk encoderä½œä¸ºåå‘æ¢¯åº¦è®­ç»ƒã€å¯¹è¯´è¯äººèº«ä»½ç‰¹å¾è¿›è¡Œè§£è€¦åˆ, æ¥æå‡å¯¹ç¾å­¦ç†è§£çš„å‡†ç¡®åº¦çš„å®éªŒéƒ¨åˆ†. 
äºŒè€…é¡¹ç›®ä»£ç æ˜¯åŒä¸€ä¸ªã€å…¼å®¹çš„. åŠ è§£è€¦éƒ¨åˆ†å› ä¸ºæœºæˆ¿é€€ç§Ÿæ—¶æ²¡æ¥å¾—åŠæ‹·è´ä¸‹æ¥, å¯¼è‡´æ¨¡å‹æƒé‡ä¸¢å¤±äº†. ä½†æ˜¯ä½¿ç”¨SaMoyeçš„spk encoderæˆ–è€…ç”¨wespeaker, è¿›è¡Œåå‘æ¢¯åº¦è®­ç»ƒæ¥è§£è€¦åˆ, çœ‹çœ‹æ•ˆæœæ˜¯å¦ä¼šå˜å¥½çš„å®éªŒ,å®éªŒç»“æœå¤§è‡´å¦‚ä¸‹: ä½¿ç”¨SaMoyeçš„spk encoderæˆ–è€… wespeakerçš„encoder, åˆ†åˆ«ä½œä¸ºåå‘æ¢¯åº¦æ¥è§£è€¦åˆ, ç„¶åæ‰“åˆ†, éƒ½ç‰¹åˆ«éš¾è®­ç»ƒã€éš¾æ”¶æ•›. ä½†æ˜¯batchsizeå°ä¸€ç‚¹ä¹Ÿèƒ½æ”¶æ•›. æœ€åè¯„ä»·ç¾å­¦ç­‰çº§çš„å‡†ç¡®ç‡, æ•ˆæœæ¯”åŸæ¥å¥½ä¸€ç‚¹ç‚¹. è¯´æ˜è¿™ä¸ªè§£è€¦éƒ¨åˆ†ç¡®å®æ˜¯æœ‰ç”¨çš„. 

æœ€åæ³¨æ„: æœ¬ä»“åº“ä¸åŒ…å«æ¨¡å‹éƒ¨åˆ†, ç ”ç©¶è€…éœ€è¦å‰å¾€huggingfaceä¸‹è½½åŒ…å«æ¨¡å‹çš„å®Œæ•´ä»“åº“ï¼š [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/]  

## ä½¿ç”¨æ–¹æ³•ï¼š  

### å®‰è£…ç¯å¢ƒ  

```bash
conda create -n audioscore python=3.10
conda activate audioscore
pip install -r requirements.txt
```

### ä¸‹è½½æ¨¡å‹  
ä¸‹è½½ä¸‹é¢çš„ç›®å½•ï¼Œæ”¾åœ¨å½“å‰ä½ç½®  
https://huggingface.co/karl-wang/QwenFeat-Vocal-Score/tree/main/audioscore/ckpts  
ç›´æ¥git clone [https://huggingface.co/karl-wang/QwenFeat-Vocal-Score](https://huggingface.co/karl-wang/QwenFeat-Vocal-Score) ä¹Ÿå¯ä»¥

### è°ƒç”¨  
(å¯æ‰§è¡Œçš„ä»£ç è§`python tests/test_generate_score.py`)  

```python
import os,sys
import audioscore.model

if __name__=="__main__":
    ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model = audioscore.model.SongEvalGenerator_audio_lora()
    model.load_model(os.path.join(ROOT_DIR,"ckpts", "SongEvalGenerator", "step_2_al_audio", "best_model_step_132000")) #åŠ è½½æ¨¡å‹
    model = model.half() #å¯é€‰
    model = model.cuda()

    score = model.generate_tag("/data/nfs/audioscore/data/sort/data/audio/203887_250822105518005501.m4a") #æ‰§è¡Œæ‰“åˆ†

    print("score:", score)

```

å¯ä»¥ç›´æ¥æ‰§è¡Œä¸‹é¢çš„ä»£ç è¿›è¡Œæµ‹è¯•  

```bash
python tests/test_generate_score.py
```

### è®­ç»ƒ  

ç›´æ¥è®­ç»ƒ  
`torchrun --nproc_per_node=4 --nnodes=1  scripts/train/train_sort_audio.py`  

å¯¹æŠ—è®­ç»ƒï¼ˆä½¿ç”¨samoyeçš„spk encoderè¿›è¡Œè§£è€¦ï¼‰  
`torchrun --nproc_per_node=4 --nnodes=1  scripts/train/train_sort_audio_grl.py`  

## å…¶ä»–æ³¨æ„äº‹é¡¹ of VocalVerse1: åŸºäºqwenaudioçš„æ­Œå”±è¯„ä»·æ¨¡å‹

condaç¯å¢ƒç”¨qwenaudio, ä¸¤ä¸ªæ¨¡å‹æƒé‡éƒ½å¤åˆ¶è¿›å»äº†

æˆ‘åŠ äº†infer_service.pyå’Œinfer.pyä¸¤ä¸ªè„šæœ¬ï¼Œä½¿ç”¨æ–¹æ³•æˆ‘å†™README.mdé‡Œé¢äº†. test/test_score.pyå¯ä»¥è¿è¡Œ

/home/w-4090/projects/qwenaudio/src/qwenaudio/prompts.py è¿™ä¸ªæ˜¯promptæ‰€åœ¨çš„ä½ç½®.

æµ‹è¯•è„šæœ¬åœ¨/home/w-4090/projects/qwenaudio/tests/test_processor_v2.pyå’Œ/home/w-4090/projects/qwenaudio/tests/test_processor_v3.py.

é™„å›¾1å’Œ2çš„ä»£ç , åŠ è½½æœ¬åœ°æ¨¡å‹æŠŠmodel.pyå’Œprocessor.pyä¸­çš„æ¨¡å‹è·¯å¾„æ”¹æˆæœ¬åœ°çš„å°±è¡Œäº†

<img width="1601" height="315" alt="77b6f8227c1d4af030fd7e2e0af8c8ab" src="https://github.com/user-attachments/assets/191e8e3f-2b67-43c0-b51e-f0b50f5d7dd6" />
<img width="1335" height="165" alt="76943b4c05d12bf090aecc602c8430b4" src="https://github.com/user-attachments/assets/8b058f65-de12-44c6-b041-e7db5ef97095" />

é™„å›¾3ä¸­ä¸¤ä¸ªæ˜¯è®­ç»ƒçš„loraæ¨¡å‹. è¿™ä¸ªæ˜¯è®­å¥½çš„. è¿™ä¸¤ä¸ªè¦ä¸Šä¼ 
<img width="418" height="56" alt="image" src="https://github.com/user-attachments/assets/e07494e7-eedc-44bc-b885-bf825de3d3dc" />

æ•´ä¸ªå·¥ç¨‹åŒ…æ‹¬ä»£ç å’Œloraæƒé‡éƒ½åœ¨é‡Œé¢ï¼Œä½†æ˜¯baseæ¨¡å‹éœ€è¦è”ç½‘ä¸‹è½½

å¦‚æœåªç”Ÿæ•ˆäº†ç»“å°¾çš„åˆ†ç±»å™¨ï¼Œæ¨¡å‹æœ¬ä½“çš„loraæ²¡ç”Ÿæ•ˆï¼Œå‡†ç¡®ç‡ä¼šå¾ˆä½ï¼›loraæƒé‡ç”Ÿæ•ˆåï¼Œå‡†ç¡®ç‡èƒ½80å¤š æ¥è¿‘9æˆ.  ç¼–ç å™¨+llmè§£ç +lora+åˆ†ç±»å™¨

é™„å›¾4 ä¸­Qwen2-Audio-7B-Instruct/è¿™ä¸ªbase æ¨¡å‹éƒ½è¦ï¼Œä¹‹å‰æ˜¯è‡ªåŠ¨ä¸‹è½½åˆ°.cacheé‡Œé¢çš„ï¼Œå•ç‹¬ä¸‹è½½çš„åº”è¯¥è¦ä»£ç é‡Œé¢é‡æ–°æŒ‡å®šè·¯å¾„

<img width="785" height="858" alt="32e73d7a89338a602f350225e859b67f" src="https://github.com/user-attachments/assets/26b3f4f9-2595-4567-934d-10d089785464" />

